{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import regex as re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # Import the stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/reddit_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>unethical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if an online video has no controls to skip, p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i always judge people's maturity based on the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  unethical\n",
       "0   if an online video has no controls to skip, p...          0\n",
       "1   i always judge people's maturity based on the...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# text_tokens = tokenizer.tokenize(text.lower())\n",
    "# tokenizer_1 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "# # Instantiate tokenizer.\n",
    "# tokenizer_2 = RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "# # Run tokenizer.\n",
    "# tokenizer_2.tokenize(s)\n",
    "\n",
    "# # Use regular expressions to do a find-and-replace\n",
    "# letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "#                       \" \",                   # The pattern to replace it with\n",
    "#                       example1.get_text())   # The text to search\n",
    "# # Convert letters_only to lower case.\n",
    "# lower_case = letters_only.lower()\n",
    "\n",
    "# # Split lower_case up at each space.\n",
    "# words = lower_case.split()  #this is tokenization with a different method!\n",
    "# words = [i for i in words if i not in stopwords.words('english')]\n",
    "\n",
    "#     # 2. Remove non-letters.\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "#     # 3. Convert to lower case, split into individual words.\n",
    "#     words = letters_only.lower().split()\n",
    "    \n",
    "#     # 4. In Python, searching a set is much faster than searching\n",
    "#     # a list, so convert the stopwords to a set.\n",
    "#     stops = set(stopwords.words('english'))\n",
    "    \n",
    "#     # 5. Remove stopwords.\n",
    "#     meaningful_words = [w for w in words if w not in stops]\n",
    "    \n",
    "#     # 6. Join the words back into one string separated by space, \n",
    "#     # and return the result.\n",
    "#     return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['unethical']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "params = {\n",
    "    'cvec__max_features': [None], \n",
    "    'cvec__stop_words': [['un', 'they', 'll', 'and']], \n",
    "    'cvec__ngram_range': [(1, 1)], \n",
    "    'cvec__min_df': [4],\n",
    "    'cvec__max_df': [.80, .70], \n",
    "    'cvec__tokenizer': [None], \n",
    "    'lr__solver': ['liblinear', 'sag', 'saga', 'newton-cg',]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(cvec_pipe,\n",
    "                   params, n_jobs=6,\n",
    "                   cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=6,\n",
       "             param_grid={'cvec__max_df': [0.8, 0.7],\n",
       "                         'cvec__max_features': [None], 'cvec__min_df': [4],\n",
       "                         'cvec__ngram_range': [(1, 1)],\n",
       "                         'cvec__stop_words': [['un', 'they', 'll', 'and']],\n",
       "                         'cvec__tokenizer': [None],\n",
       "                         'lr__solver': ['liblinear', 'sag', 'saga',\n",
       "                                        'newton-cg']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8035463288841044"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9539257169722614"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8057626435623615"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.append(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.append(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 4,\n",
       "  'cvec__ngram_range': (1, 2),\n",
       "  'cvec__stop_words': None},\n",
       " 0.802806720789541,\n",
       " {'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 1000,\n",
       "  'cvec__min_df': 5,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': None,\n",
       "  'cvec__tokenizer': None},\n",
       " 0.7715084508430765,\n",
       " {'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 1000,\n",
       "  'cvec__min_df': 5,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they'],\n",
       "  'cvec__tokenizer': None},\n",
       " 0.7687550603428615,\n",
       " {'cvec__max_df': 0.8,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 3,\n",
       "  'cvec__ngram_range': (1, 2),\n",
       "  'cvec__stop_words': ['un', 'they'],\n",
       "  'cvec__tokenizer': None},\n",
       " 0.7710390811308458,\n",
       " {'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 3,\n",
       "  'cvec__ngram_range': (1, 2),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None},\n",
       " 0.7702331936834739,\n",
       " {'cvec__max_df': 0.8,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 5,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None},\n",
       " 0.7666734419896619,\n",
       " {'cvec__max_df': 0.8,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 4,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None},\n",
       " 0.7668076926460825,\n",
       " {'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 3,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None},\n",
       " 0.7967629597227728,\n",
       " {'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 3,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None,\n",
       "  'lr__solver': 'lbfgs'},\n",
       " 0.7969643921057636,\n",
       " {'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 3,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None,\n",
       "  'lr__solver': 'lbfgs'},\n",
       " 0.7969643921057636,\n",
       " {'cvec__max_df': 0.7,\n",
       "  'cvec__max_features': 5000,\n",
       "  'cvec__min_df': 3,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None,\n",
       "  'lr__solver': 'newton-cg'},\n",
       " 0.796830141449343,\n",
       " {'cvec__max_df': 0.8,\n",
       "  'cvec__max_features': None,\n",
       "  'cvec__min_df': 4,\n",
       "  'cvec__ngram_range': (1, 1),\n",
       "  'cvec__stop_words': ['un', 'they', 'll', 'and'],\n",
       "  'cvec__tokenizer': None,\n",
       "  'lr__solver': 'liblinear'},\n",
       " 0.8035463288841044]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9358 coefficients.\n",
      "There are 9358 features.\n"
     ]
    }
   ],
   "source": [
    "coefficients = grid.best_estimator_.named_steps['lr'].coef_[0]\n",
    "\n",
    "\n",
    "features = grid.best_estimator_.named_steps['cvec'].get_feature_names()\n",
    "\n",
    "\n",
    "print(f'There are {len(coefficients)} coefficients.')\n",
    "print(f'There are {len(features)} features.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'features': features, \n",
    "              'coef' : coefficients,\n",
    "              'exp_coef': [np.exp(coef) for coef in coefficients] #exponentiated coefficients\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>piss</th>\n",
       "      <td>2.107159</td>\n",
       "      <td>8.224842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1.923127</td>\n",
       "      <td>6.842318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halloween</th>\n",
       "      <td>1.778441</td>\n",
       "      <td>5.920621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rob</th>\n",
       "      <td>1.748464</td>\n",
       "      <td>5.745768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exam</th>\n",
       "      <td>1.741647</td>\n",
       "      <td>5.706737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lie</th>\n",
       "      <td>1.726047</td>\n",
       "      <td>5.618401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fart</th>\n",
       "      <td>1.704332</td>\n",
       "      <td>5.497711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drunk</th>\n",
       "      <td>1.651251</td>\n",
       "      <td>5.213499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>murder</th>\n",
       "      <td>1.645804</td>\n",
       "      <td>5.185179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>1.610234</td>\n",
       "      <td>5.003981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               coef  exp_coef\n",
       "features                     \n",
       "piss       2.107159  8.224842\n",
       "fake       1.923127  6.842318\n",
       "halloween  1.778441  5.920621\n",
       "rob        1.748464  5.745768\n",
       "exam       1.741647  5.706737\n",
       "lie        1.726047  5.618401\n",
       "fart       1.704332  5.497711\n",
       "drunk      1.651251  5.213499\n",
       "murder     1.645804  5.185179\n",
       "claim      1.610234  5.003981"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df = coef_df.set_index('features')\n",
    "coef_df = coef_df.sort_values('exp_coef', ascending = False)\n",
    "coef_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tp</th>\n",
       "      <td>-1.460031</td>\n",
       "      <td>0.232229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spelling</th>\n",
       "      <td>-1.492563</td>\n",
       "      <td>0.224796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>okay</th>\n",
       "      <td>-1.510168</td>\n",
       "      <td>0.220873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selection</th>\n",
       "      <td>-1.516074</td>\n",
       "      <td>0.219572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germs</th>\n",
       "      <td>-1.523499</td>\n",
       "      <td>0.217948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happier</th>\n",
       "      <td>-1.581056</td>\n",
       "      <td>0.205758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>published</th>\n",
       "      <td>-1.628742</td>\n",
       "      <td>0.196176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pets</th>\n",
       "      <td>-1.648059</td>\n",
       "      <td>0.192423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needles</th>\n",
       "      <td>-1.720076</td>\n",
       "      <td>0.179053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unexpectedly</th>\n",
       "      <td>-1.753172</td>\n",
       "      <td>0.173224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  exp_coef\n",
       "features                        \n",
       "tp           -1.460031  0.232229\n",
       "spelling     -1.492563  0.224796\n",
       "okay         -1.510168  0.220873\n",
       "selection    -1.516074  0.219572\n",
       "germs        -1.523499  0.217948\n",
       "happier      -1.581056  0.205758\n",
       "published    -1.628742  0.196176\n",
       "pets         -1.648059  0.192423\n",
       "needles      -1.720076  0.179053\n",
       "unexpectedly -1.753172  0.173224"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
